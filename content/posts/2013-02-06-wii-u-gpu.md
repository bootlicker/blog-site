---
title: Wii U GPU
author: admin
type: post
date: 2013-02-06T10:01:25+00:00
url: /wii-u/wii-u-gpu/
al2fb_facebook_link_id:
  - 552216077_10151249167726078
al2fb_facebook_link_time:
  - 2013-02-06T10:01:30+00:00
al2fb_facebook_link_picture:
  - featured=http://jumpnshoot9000.com/wp-content/uploads/2013/02/wii-u-gpu-150x150.jpg
original_post_id:
  - 393
categories:
  - Wii U

---
<img class="wp-image-397 alignleft" alt="" src="http://jumpnshoot9000.com/wp-content/uploads/2013/02/DSC_9039sm1-300x160.jpg" width="216" height="115" />Efforts over at Chipworks.com (prompted by a crowd-sourcing [initiative][1] to buy photos from them over at community forum NeoGAF.com) have resulted in very special microscopic photos of the semiconductor circuitry of the Wii U&#8217;s Graphics Processing Unit (vastly superior in detail to those [previously posted][2] on Anandtech.com).

As the article over at [Eurogamer][3] reads:

> It was ballpark speculation at the time based on what we had eyeballed at the event, but the final GPU is indeed a close match to the [Radeon HD] 4650/4670, albeit with a deficit in the number of texture-mapping units and a lower clock speed &#8211; 550MHz. AMD&#8217;s RV770 hardware is well documented so with these numbers we can now, categorically, finally rule out any next-gen pretensions for the Wii U &#8211; the GCN hardware in Durango and Orbis is in a completely different league. However, the 16 TMUs at 550MHz and texture cache improvements found in RV770 do elevate the capabilities of this hardware beyond the Xenos GPU in the Xbox 360 &#8211; 1.5 times the raw shader power sounds about right. 1080p resolution is around 2.5x that of 720p, so bearing in mind the inclusion of just eight ROPs, it&#8217;s highly unlikely that we&#8217;ll be seeing any complex 3D titles running at 1080p.
> 
> All of which may lead some to wonder quite why many of the Wii U ports disappoint &#8211; especially Black Ops 2, which appears to have been derived from the Xbox 360 version, running more slowly even at the same 880&#215;720 sub-hd resolution. The answer comes from a mixture of known and unknown variables.
> 
> The obvious suspect would be the Wii U&#8217;s 1.2GHz CPU, a tri-core piece of hardware re-architected from the Wii&#8217;s Broadway chip, in turn a tweaked, overclocked version of the GameCube&#8217;s Gekko processor. In many of our Wii U Face-Offs we&#8217;ve seen substantial performance dips on CPU-specific tasks. However, there still plenty of unknowns to factor in too &#8211; specifically the bandwidth levels from the main RAM and the exact nature of the GPU&#8217;s interface to its 32MB of onboard eDRAM. While the general capabilities of the Wii U hardware are now beyond doubt, discussion will continue about how the principal processing elements and the memory are interfaced together, and Nintendo&#8217;s platform-exclusive titles should give us some indication of what this core is capable of when developers are targeting it directly.

While it&#8217;s obviously true that video game excellence is independent of graphical prowess, many were expecting the Wii U to cater more towards mainstream (FPS) gamers with its hardware&#8211;and to a certain extent, moves towards this have obviously been made&#8211;but, as has been mentioned on the NeoGAF thread above, Nintendo&#8217;s decision to make the Wii U backwards compatible has come at a cost in performance. Also worth mentioning is the console&#8217;s relatively unconventional memory organisation &#8211; both the CPU and the GPU have  common access to all of the system&#8217;s RAM.

 [1]: http://www.neogaf.com/forum/showthread.php?t=500466
 [2]: http://www.anandtech.com/show/6465/nintendo-wii-u-teardown
 [3]: http://www.eurogamer.net/articles/df-hardware-wii-u-graphics-power-finally-revealed